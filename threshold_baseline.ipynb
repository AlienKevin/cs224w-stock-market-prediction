{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch_geometric_temporal.signal import StaticGraphTemporalSignal, DynamicGraphTemporalSignal\n",
    "import torch\n",
    "from typing import Union\n",
    "import glob\n",
    "from natsort import natsorted\n",
    "\n",
    "class SP500CorrelationsDatasetLoader(object):\n",
    "    def __init__(self, corr_name, corr_scope):\n",
    "        self._read_csv(corr_name, corr_scope)\n",
    "\n",
    "    def _read_csv(self, corr_name, corr_scope):\n",
    "        match corr_scope:\n",
    "            case 'global':\n",
    "                self._correlation_matrices = [np.loadtxt(f'{corr_name}/{corr_scope}_corr.csv', delimiter=',')]\n",
    "            case 'local':\n",
    "                self._correlation_matrices = []\n",
    "                corr_files = natsorted(glob.glob(f'{corr_name}/local_corr_*.csv'))\n",
    "                for corr_file in corr_files:\n",
    "                    matrix = np.loadtxt(corr_file, delimiter=',')\n",
    "                    self._correlation_matrices.append(matrix)\n",
    "        \n",
    "        # Thesholding\n",
    "        _correlation_threshold = 0.9\n",
    "        for matrix in self._correlation_matrices:\n",
    "            matrix[matrix < _correlation_threshold] = 0\n",
    "        \n",
    "        df = pd.read_csv('s&p500.csv')\n",
    "        df = df.set_index('Date')\n",
    "        data = torch.from_numpy(df.to_numpy()).to(torch.float32)\n",
    "\n",
    "        # Round data size to nearest multiple of batch_size\n",
    "        self.days_in_quarter = 64\n",
    "        num_quarters = data.size(0) // self.days_in_quarter\n",
    "        num_days = num_quarters * self.days_in_quarter\n",
    "        data = data[:num_days]\n",
    "        \n",
    "        # z-score normalization with training data following GERU\n",
    "        train_days = int(0.8 * num_quarters) * self.days_in_quarter\n",
    "        data = (data - data[:train_days].mean(dim=0)) / data[:train_days].std(dim=0)\n",
    "        data = data.numpy()\n",
    "        np.savetxt('s&p500_z_scores.csv', data, delimiter=',')\n",
    "        self._dataset = data\n",
    "\n",
    "    def _get_edges(self):\n",
    "        if len(self._correlation_matrices) == 1:\n",
    "            self._edges = np.array(self._correlation_matrix.nonzero())\n",
    "        else:\n",
    "            self._edges = []\n",
    "            for time in range(self._dataset.shape[0] - self.lags):\n",
    "                corr_index = time // self.days_in_quarter\n",
    "                self._edges.append(\n",
    "                    np.array(self._correlation_matrices[corr_index].nonzero())\n",
    "                )\n",
    "\n",
    "    def _get_edge_weights(self):\n",
    "        if len(self._correlation_matrices) == 1:\n",
    "            self._edge_weights = self._correlation_matrix[self._correlation_matrix > 0]\n",
    "        else:\n",
    "            self._edge_weights = []\n",
    "            for time in range(self._dataset.shape[0] - self.lags):\n",
    "                corr_index = time // self.days_in_quarter\n",
    "                self._edge_weights.append(\n",
    "                    np.array(self._correlation_matrices[corr_index][self._correlation_matrices[corr_index] > 0])\n",
    "                )\n",
    "\n",
    "    def _get_targets_and_features(self):\n",
    "        stacked_target = self._dataset\n",
    "        self.features = [\n",
    "            stacked_target[i : i + self.lags, :].T\n",
    "            for i in range(stacked_target.shape[0] - self.lags)\n",
    "        ]\n",
    "        # predict next-day stock movement\n",
    "        self.targets = [\n",
    "            ((stacked_target[i + self.lags, :] > stacked_target[i + self.lags - 1, :]).astype(float)).T\n",
    "            for i in range(stacked_target.shape[0] - self.lags)\n",
    "        ]\n",
    "\n",
    "    def get_dataset(self, lags: int) -> Union[StaticGraphTemporalSignal, DynamicGraphTemporalSignal]:\n",
    "        \"\"\"Returning the data iterator.\n",
    "\n",
    "        Args types:\n",
    "            * **lags** *(int)* - The number of time lags.\n",
    "        Return types:\n",
    "            * **dataset**\n",
    "        \"\"\"\n",
    "        self.lags = lags\n",
    "        self._get_edges()\n",
    "        self._get_edge_weights()\n",
    "        self._get_targets_and_features()\n",
    "        dataset = (DynamicGraphTemporalSignal if type(self._edges) == list else StaticGraphTemporalSignal)(\n",
    "            self._edges, self._edge_weights, self.features, self.targets\n",
    "        )\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric_temporal.signal import temporal_signal_split\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = 'cpu'\n",
    "\n",
    "corr_name = 'mi'\n",
    "corr_scope = 'local'\n",
    "loader = SP500CorrelationsDatasetLoader(corr_name=corr_name, corr_scope=corr_scope)\n",
    "\n",
    "lags = 63\n",
    "\n",
    "dataset = loader.get_dataset(lags)\n",
    "\n",
    "train_dataset, test_val_dataset = temporal_signal_split(dataset, train_ratio=0.8)\n",
    "val_dataset, test_dataset = temporal_signal_split(test_val_dataset, train_ratio=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torcheval.metrics.functional import binary_f1_score, binary_accuracy\n",
    "\n",
    "def accuracy(y_hats, ys):\n",
    "    return binary_accuracy(y_hats.flatten(), ys.flatten()).item()\n",
    "\n",
    "def f1(y_hats, ys):\n",
    "    return binary_f1_score(y_hats.flatten(), ys.flatten()).item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric_temporal.nn.recurrent import DCRNN\n",
    "\n",
    "class RecurrentGCN(torch.nn.Module):\n",
    "    def __init__(self, node_features):\n",
    "        super(RecurrentGCN, self).__init__()\n",
    "        self.recurrent = DCRNN(node_features, 64, 1)\n",
    "        self.linear = torch.nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        h = self.recurrent(x, edge_index, edge_weight)\n",
    "        h = F.relu(h)\n",
    "        return F.sigmoid(self.linear(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:diw9q5rj) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">crisp-dew-89</strong> at: <a href='https://wandb.ai/kevinxli/cs224w-stock-market-prediction/runs/diw9q5rj' target=\"_blank\">https://wandb.ai/kevinxli/cs224w-stock-market-prediction/runs/diw9q5rj</a><br/> View project at: <a href='https://wandb.ai/kevinxli/cs224w-stock-market-prediction' target=\"_blank\">https://wandb.ai/kevinxli/cs224w-stock-market-prediction</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241206_201202-diw9q5rj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:diw9q5rj). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Volumes/crucialx9/classes/fall2024/cs224w/stock/wandb/run-20241206_201255-wen6ge3v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kevinxli/cs224w-stock-market-prediction/runs/wen6ge3v' target=\"_blank\">crimson-spaceship-90</a></strong> to <a href='https://wandb.ai/kevinxli/cs224w-stock-market-prediction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kevinxli/cs224w-stock-market-prediction' target=\"_blank\">https://wandb.ai/kevinxli/cs224w-stock-market-prediction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kevinxli/cs224w-stock-market-prediction/runs/wen6ge3v' target=\"_blank\">https://wandb.ai/kevinxli/cs224w-stock-market-prediction/runs/wen6ge3v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:17<05:39, 17.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, val/acc: 0.4903396666049957, val/f1: 0.16911618411540985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:34<05:05, 16.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, val/acc: 0.4904443025588989, val/f1: 0.24367834627628326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:49<04:38, 16.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, val/acc: 0.49076688289642334, val/f1: 0.2539946138858795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [01:06<04:21, 16.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, val/acc: 0.49057507514953613, val/f1: 0.2839846611022949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [01:22<04:04, 16.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, val/acc: 0.49103718996047974, val/f1: 0.28707167506217957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [01:38<03:49, 16.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, val/acc: 0.5115174055099487, val/f1: 0.6557459831237793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [01:54<03:30, 16.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, val/acc: 0.5113255977630615, val/f1: 0.6567286849021912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [02:11<03:16, 16.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, val/acc: 0.5127031207084656, val/f1: 0.6723107695579529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [02:27<02:57, 16.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, val/acc: 0.49200496077537537, val/f1: 0.34429827332496643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [02:43<02:40, 16.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, val/acc: 0.5106106400489807, val/f1: 0.6657376289367676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [02:59<02:25, 16.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, val/acc: 0.5114476680755615, val/f1: 0.6644791960716248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [03:15<02:10, 16.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, val/acc: 0.4898863136768341, val/f1: 0.2257978916168213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [03:31<01:53, 16.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, val/acc: 0.4903396666049957, val/f1: 0.23786179721355438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [03:47<01:36, 16.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, val/acc: 0.5123195052146912, val/f1: 0.645210862159729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [04:03<01:20, 16.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, val/acc: 0.49178698658943176, val/f1: 0.2299460917711258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [04:18<01:01, 15.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, val/acc: 0.49152541160583496, val/f1: 0.22553914785385132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [04:30<00:43, 14.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, val/acc: 0.49137720465660095, val/f1: 0.24801163375377655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [04:42<00:27, 13.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, val/acc: 0.49156901240348816, val/f1: 0.24170707166194916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [04:55<00:13, 13.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, val/acc: 0.49099358916282654, val/f1: 0.2509590685367584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:07<00:00, 15.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, val/acc: 0.49124643206596375, val/f1: 0.27127406001091003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "model = RecurrentGCN(node_features = lags).to(device)\n",
    "\n",
    "lr = 1e-3\n",
    "num_epochs = 20\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "track_with_wandb = True\n",
    "\n",
    "if track_with_wandb:\n",
    "    wandb.init(project=\"cs224w-stock-market-prediction\", config={\n",
    "        \"dataset\": \"S&P500\",\n",
    "        \"corr_name\": corr_name,\n",
    "        \"corr_scope\": corr_scope,\n",
    "        \"learning_rate\": lr,\n",
    "        \"epochs\": num_epochs,\n",
    "        \"architecture\": \"DCRNN\",\n",
    "    })\n",
    "\n",
    "best_f1 = 0\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for time, snapshot in enumerate(train_dataset):\n",
    "        y_hat = model(snapshot.x.to(device), snapshot.edge_index.to(device), snapshot.edge_attr.to(device))\n",
    "        loss = F.binary_cross_entropy(y_hat.squeeze(), snapshot.y)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    train_loss /= (time+1)\n",
    "\n",
    "    if track_with_wandb:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_y_hats, val_ys = zip(*[(model(snapshot.x.to(device), snapshot.edge_index.to(device), snapshot.edge_attr.to(device)), snapshot.y)\n",
    "                       for time, snapshot in enumerate(val_dataset)])\n",
    "            val_y_hats, val_ys = torch.stack(list(val_y_hats)).squeeze().to(device), torch.stack(list(val_ys)).to(device)\n",
    "            val_acc = accuracy(val_y_hats, val_ys)\n",
    "            val_f1 = f1(val_y_hats, val_ys)\n",
    "            wandb.log({\"epoch\": epoch,\n",
    "                    \"train/loss\": train_loss,\n",
    "                    \"val/acc\": val_acc,\n",
    "                    \"val/f1\": val_f1 })\n",
    "            print(f'Epoch {epoch}, val/acc: {val_acc}, val/f1: {val_f1}')\n",
    "            if val_f1 > best_f1:\n",
    "                best_f1 = val_f1\n",
    "                torch.save(model.state_dict(), 'dcrnn_best_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if track_with_wandb:\n",
    "    best_model = RecurrentGCN(node_features = lags).to(device)\n",
    "    best_model.load_state_dict(torch.load('dcrnn_best_model.pth', weights_only=True))\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_hats, ys = zip(*[(best_model(snapshot.x.to(device), snapshot.edge_index.to(device), snapshot.edge_attr.to(device)), snapshot.y)\n",
    "                       for time, snapshot in enumerate(test_dataset)])\n",
    "        y_hats, ys = torch.stack(list(y_hats)).squeeze().to(device), torch.stack(list(ys)).to(device)\n",
    "        wandb.log({\"epoch\": epoch,\n",
    "                \"test/acc\": accuracy(y_hats, ys),\n",
    "                \"test/f1\": f1(y_hats, ys) })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
