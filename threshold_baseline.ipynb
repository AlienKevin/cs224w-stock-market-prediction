{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch_geometric_temporal.signal import StaticGraphTemporalSignalBatch, DynamicGraphTemporalSignalBatch\n",
    "import torch\n",
    "from typing import Union\n",
    "import glob\n",
    "from natsort import natsorted\n",
    "\n",
    "class SP500CorrelationsDatasetLoader(object):\n",
    "    def __init__(self, corr_name, corr_scope):\n",
    "        self._read_csv(corr_name, corr_scope)\n",
    "\n",
    "    def _read_csv(self, corr_name, corr_scope):\n",
    "        match corr_scope:\n",
    "            case 'global':\n",
    "                self._correlation_matrices = [np.loadtxt(f'{corr_name}/{corr_scope}_corr.csv', delimiter=',')]\n",
    "            case 'local':\n",
    "                self._correlation_matrices = []\n",
    "                corr_files = natsorted(glob.glob(f'{corr_name}/local_corr_*.csv'))\n",
    "                for corr_file in corr_files:\n",
    "                    matrix = np.loadtxt(corr_file, delimiter=',')\n",
    "                    self._correlation_matrices.append(matrix)\n",
    "        \n",
    "        # Thesholding\n",
    "        _correlation_threshold = 0.9\n",
    "        for matrix in self._correlation_matrices:\n",
    "            matrix[matrix < _correlation_threshold] = 0\n",
    "        \n",
    "        df = pd.read_csv('s&p500.csv')\n",
    "        df = df.set_index('Date')\n",
    "        data = torch.from_numpy(df.to_numpy()).to(torch.float32)\n",
    "\n",
    "        # Round data size to nearest multiple of batch_size\n",
    "        self.days_in_quarter = 64\n",
    "        num_quarters = data.size(0) // self.days_in_quarter\n",
    "        num_days = num_quarters * self.days_in_quarter\n",
    "        data = data[:num_days]\n",
    "        \n",
    "        # z-score normalization with training data following GERU\n",
    "        train_days = int(0.8 * num_quarters) * self.days_in_quarter\n",
    "        data = (data - data[:train_days].mean(dim=0)) / data[:train_days].std(dim=0)\n",
    "        data = data.numpy()\n",
    "        np.savetxt('s&p500_z_scores.csv', data, delimiter=',')\n",
    "        self._dataset = data\n",
    "\n",
    "    def _get_edges(self):\n",
    "        if len(self._correlation_matrices) == 1:\n",
    "            self._edges = np.array(self._correlation_matrix.nonzero())\n",
    "        else:\n",
    "            self._edges = []\n",
    "            for time in range(self._dataset.shape[0] - self.lags):\n",
    "                corr_index = time // self.days_in_quarter\n",
    "                self._edges.append(\n",
    "                    np.array(self._correlation_matrices[corr_index].nonzero())\n",
    "                )\n",
    "\n",
    "    def _get_edge_weights(self):\n",
    "        if len(self._correlation_matrices) == 1:\n",
    "            self._edge_weights = self._correlation_matrix[self._correlation_matrix > 0]\n",
    "        else:\n",
    "            self._edge_weights = []\n",
    "            for time in range(self._dataset.shape[0] - self.lags):\n",
    "                corr_index = time // self.days_in_quarter\n",
    "                self._edge_weights.append(\n",
    "                    np.array(self._correlation_matrices[corr_index][self._correlation_matrices[corr_index] > 0])\n",
    "                )\n",
    "\n",
    "    def _get_targets_and_features(self):\n",
    "        stacked_target = self._dataset\n",
    "        self.features = [\n",
    "            stacked_target[i : i + self.lags, :].T\n",
    "            for i in range(stacked_target.shape[0] - self.lags)\n",
    "        ]\n",
    "        # predict next-day stock movement\n",
    "        self.targets = [\n",
    "            ((stacked_target[i + self.lags, :] > stacked_target[i + self.lags - 1, :]).astype(float)).T\n",
    "            for i in range(stacked_target.shape[0] - self.lags)\n",
    "        ]\n",
    "\n",
    "    def get_dataset(self) -> Union[StaticGraphTemporalSignalBatch, DynamicGraphTemporalSignalBatch]:\n",
    "        \"\"\"Returning the data iterator.\n",
    "        \"\"\"\n",
    "        self.lags = self.days_in_quarter\n",
    "        self._get_edges()\n",
    "        self._get_edge_weights()\n",
    "        self._get_targets_and_features()\n",
    "        self.batches = np.repeat(np.arange((self._dataset.shape[0] - self.lags) // self.days_in_quarter), self.days_in_quarter)\n",
    "        dataset = (DynamicGraphTemporalSignalBatch if type(self._edges) == list else StaticGraphTemporalSignalBatch)(\n",
    "            self._edges, self._edge_weights, self.features, self.targets, self.batches\n",
    "        )\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric_temporal.signal import temporal_signal_split\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = 'cpu'\n",
    "\n",
    "corr_name = 'mi'\n",
    "corr_scope = 'local'\n",
    "loader = SP500CorrelationsDatasetLoader(corr_name=corr_name, corr_scope=corr_scope)\n",
    "\n",
    "dataset = loader.get_dataset()\n",
    "lags = loader.lags\n",
    "\n",
    "train_dataset, test_val_dataset = temporal_signal_split(dataset, train_ratio=0.8)\n",
    "val_dataset, test_dataset = temporal_signal_split(test_val_dataset, train_ratio=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torcheval.metrics.functional import binary_f1_score, binary_accuracy\n",
    "\n",
    "def accuracy(y_hats, ys):\n",
    "    return binary_accuracy(y_hats.flatten(), ys.flatten()).item()\n",
    "\n",
    "def f1(y_hats, ys):\n",
    "    return binary_f1_score(y_hats.flatten(), ys.flatten()).item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric_temporal.nn.recurrent import DCRNN\n",
    "\n",
    "class RecurrentGCN(torch.nn.Module):\n",
    "    def __init__(self, node_features):\n",
    "        super(RecurrentGCN, self).__init__()\n",
    "        self.recurrent = DCRNN(node_features, 64, 1)\n",
    "        self.linear = torch.nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        h = self.recurrent(x, edge_index, edge_weight)\n",
    "        h = F.relu(h)\n",
    "        return F.sigmoid(self.linear(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:wen6ge3v) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇███</td></tr><tr><td>test/acc</td><td>▁</td></tr><tr><td>test/f1</td><td>▁</td></tr><tr><td>train/loss</td><td>█▄▃▂▃▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>val/acc</td><td>▁▁▁▁▁███▂▇█▁▁█▂▂▁▂▁▁</td></tr><tr><td>val/f1</td><td>▁▂▂▃▃███▃██▂▂█▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>19</td></tr><tr><td>test/acc</td><td>0.53708</td></tr><tr><td>test/f1</td><td>0.69185</td></tr><tr><td>train/loss</td><td>0.69197</td></tr><tr><td>val/acc</td><td>0.49125</td></tr><tr><td>val/f1</td><td>0.27127</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">crimson-spaceship-90</strong> at: <a href='https://wandb.ai/kevinxli/cs224w-stock-market-prediction/runs/wen6ge3v' target=\"_blank\">https://wandb.ai/kevinxli/cs224w-stock-market-prediction/runs/wen6ge3v</a><br/> View project at: <a href='https://wandb.ai/kevinxli/cs224w-stock-market-prediction' target=\"_blank\">https://wandb.ai/kevinxli/cs224w-stock-market-prediction</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241206_201255-wen6ge3v/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:wen6ge3v). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Volumes/crucialx9/classes/fall2024/cs224w/stock/wandb/run-20241206_204202-a2y9a11j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kevinxli/cs224w-stock-market-prediction/runs/a2y9a11j' target=\"_blank\">woven-jazz-91</a></strong> to <a href='https://wandb.ai/kevinxli/cs224w-stock-market-prediction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kevinxli/cs224w-stock-market-prediction' target=\"_blank\">https://wandb.ai/kevinxli/cs224w-stock-market-prediction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kevinxli/cs224w-stock-market-prediction/runs/a2y9a11j' target=\"_blank\">https://wandb.ai/kevinxli/cs224w-stock-market-prediction/runs/a2y9a11j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:10<03:25, 10.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, val/acc: 0.5114389061927795, val/f1: 0.6593183279037476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:23<03:34, 11.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, val/acc: 0.4902612268924713, val/f1: 0.23769477009773254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:36<03:30, 12.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, val/acc: 0.5117702484130859, val/f1: 0.6603999733924866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:48<03:18, 12.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, val/acc: 0.4902088940143585, val/f1: 0.2013576626777649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [01:01<03:07, 12.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, val/acc: 0.5118748545646667, val/f1: 0.6542878746986389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [01:13<02:54, 12.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, val/acc: 0.5114215016365051, val/f1: 0.655341625213623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [01:25<02:39, 12.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, val/acc: 0.5115784406661987, val/f1: 0.655494749546051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [01:38<02:28, 12.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, val/acc: 0.5117179155349731, val/f1: 0.6622521281242371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [01:51<02:17, 12.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, val/acc: 0.5113255977630615, val/f1: 0.6542853116989136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [02:02<02:02, 12.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, val/acc: 0.5117179155349731, val/f1: 0.6616277098655701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [02:14<01:48, 12.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, val/acc: 0.49080178141593933, val/f1: 0.2851267158985138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [02:25<01:35, 11.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, val/acc: 0.5110989212989807, val/f1: 0.6507166028022766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [02:38<01:23, 11.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, val/acc: 0.5112035274505615, val/f1: 0.6653294563293457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [02:50<01:12, 12.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, val/acc: 0.49496060609817505, val/f1: 0.39971813559532166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [03:03<01:01, 12.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, val/acc: 0.5111250877380371, val/f1: 0.6674337387084961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [03:14<00:48, 12.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, val/acc: 0.5113255977630615, val/f1: 0.6674261689186096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [03:26<00:35, 11.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, val/acc: 0.5110030174255371, val/f1: 0.6607674360275269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [03:38<00:24, 12.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, val/acc: 0.5108983516693115, val/f1: 0.6506843566894531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [03:51<00:12, 12.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, val/acc: 0.5118051171302795, val/f1: 0.6670867800712585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [04:03<00:00, 12.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, val/acc: 0.5117789506912231, val/f1: 0.6646665334701538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "model = RecurrentGCN(node_features = lags).to(device)\n",
    "\n",
    "lr = 1e-3\n",
    "num_epochs = 20\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "track_with_wandb = True\n",
    "\n",
    "if track_with_wandb:\n",
    "    wandb.init(project=\"cs224w-stock-market-prediction\", config={\n",
    "        \"dataset\": \"S&P500\",\n",
    "        \"corr_name\": corr_name,\n",
    "        \"corr_scope\": corr_scope,\n",
    "        \"learning_rate\": lr,\n",
    "        \"epochs\": num_epochs,\n",
    "        \"architecture\": \"DCRNN\",\n",
    "    })\n",
    "\n",
    "best_f1 = 0\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for time, snapshot in enumerate(train_dataset):\n",
    "        y_hat = model(snapshot.x.to(device), snapshot.edge_index.to(device), snapshot.edge_attr.to(device))\n",
    "        loss = F.binary_cross_entropy(y_hat.squeeze(), snapshot.y)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    train_loss /= (time+1)\n",
    "\n",
    "    if track_with_wandb:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_y_hats, val_ys = zip(*[(model(snapshot.x.to(device), snapshot.edge_index.to(device), snapshot.edge_attr.to(device)), snapshot.y)\n",
    "                       for time, snapshot in enumerate(val_dataset)])\n",
    "            val_y_hats, val_ys = torch.stack(list(val_y_hats)).squeeze().to(device), torch.stack(list(val_ys)).to(device)\n",
    "            val_acc = accuracy(val_y_hats, val_ys)\n",
    "            val_f1 = f1(val_y_hats, val_ys)\n",
    "            wandb.log({\"epoch\": epoch,\n",
    "                    \"train/loss\": train_loss,\n",
    "                    \"val/acc\": val_acc,\n",
    "                    \"val/f1\": val_f1 })\n",
    "            print(f'Epoch {epoch}, val/acc: {val_acc}, val/f1: {val_f1}')\n",
    "            if val_f1 > best_f1:\n",
    "                best_f1 = val_f1\n",
    "                torch.save(model.state_dict(), 'dcrnn_best_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "if track_with_wandb:\n",
    "    best_model = RecurrentGCN(node_features = lags).to(device)\n",
    "    best_model.load_state_dict(torch.load('dcrnn_best_model.pth', weights_only=True))\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_hats, ys = zip(*[(best_model(snapshot.x.to(device), snapshot.edge_index.to(device), snapshot.edge_attr.to(device)), snapshot.y)\n",
    "                       for time, snapshot in enumerate(test_dataset)])\n",
    "        y_hats, ys = torch.stack(list(y_hats)).squeeze().to(device), torch.stack(list(ys)).to(device)\n",
    "        wandb.log({\"epoch\": epoch,\n",
    "                \"test/acc\": accuracy(y_hats, ys),\n",
    "                \"test/f1\": f1(y_hats, ys) })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
